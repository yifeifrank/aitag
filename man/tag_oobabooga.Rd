% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/obabooga_api_call.R
\name{tag_oobabooga}
\alias{tag_oobabooga}
\title{Annotate Text Data Using Oobabooga's Models}
\usage{
tag_oobabooga(
  user_prompt,
  sys_prompt = "",
  max_new_tokens = 250,
  do_sample = TRUE,
  temperature = 1,
  top_p = 0.9,
  typical_p = 1,
  repetition_penalty = 1.05,
  encoder_repetition_penalty = 1,
  top_k = 0,
  min_length = 0,
  no_repeat_ngram_size = 0,
  num_beams = 1,
  penalty_alpha = 0,
  length_penalty = 1,
  early_stopping = FALSE,
  seed = -1,
  add_bos_token = FALSE,
  truncation_length = 2048,
  ban_eos_token = FALSE,
  skip_special_tokens = FALSE,
  api_url = "http://localhost:5000/api/v1/generate",
  verbose = TRUE
)
}
\arguments{
\item{user_prompt}{A character vector containing the text data to be annotated.}

\item{sys_prompt}{A character string containing the instruction for the Oobabooga model to follow when annotating the text data. Default is an empty string.}

\item{max_new_tokens}{The maximum number of tokens to generate in the response. Default is 250.}

\item{do_sample}{Whether to use sampling or not. If set to FALSE, the model will use greedy decoding. Default is TRUE.}

\item{temperature}{Numeric. The temperature parameter for the Oobabooga model, controlling the randomness of the generated annotations. Default is 1.}

\item{top_p}{Numeric. The top-p parameter for the Oobabooga model, controlling the cumulative probability threshold for token selection. Default is 0.9.}

\item{typical_p}{Numeric. The typical-p parameter for the Oobabooga model, controlling the typical sampling technique. Default is 1.}

\item{repetition_penalty}{Numeric. The repetition penalty parameter for the Oobabooga model, controlling the penalty for repeating tokens. Default is 1.05.}

\item{encoder_repetition_penalty}{Numeric. The encoder repetition penalty parameter for the Oobabooga model. Default is 1.}

\item{top_k}{Integer. The top-k parameter for the Oobabooga model, controlling the number of top tokens to consider for sampling. Default is 0.}

\item{min_length}{Integer. The minimum length of the generated response. Default is 0.}

\item{no_repeat_ngram_size}{Integer. The size of the n-grams to avoid repeating in the generated response. Default is 0.}

\item{num_beams}{Integer. The number of beams to use for beam search. Default is 1.}

\item{penalty_alpha}{Numeric. The length penalty parameter for the Oobabooga model. Default is 0.}

\item{length_penalty}{Numeric. The exponential length penalty parameter for the Oobabooga model. Default is 1.}

\item{early_stopping}{Whether to stop the generation when all beams reach the end of the sequence. Default is FALSE.}

\item{seed}{Integer. The random seed for reproducibility. Default is -1.}

\item{add_bos_token}{Whether to add the BOS (beginning of sequence) token to the generated response. Default is FALSE.}

\item{truncation_length}{Integer. The maximum length of the input sequence. Default is 2048.}

\item{ban_eos_token}{Whether to ban the EOS (end of sequence) token from being generated. Default is FALSE.}

\item{skip_special_tokens}{Whether to skip special tokens in the generated response. Default is FALSE.}

\item{api_url}{The URL for the Oobabooga API endpoint. Default is "http://localhost:5000/api/v1/generate".}

\item{verbose}{Logical. If TRUE, prints progress and error messages. Default is TRUE.}
}
\value{
A character vector containing the annotated responses from the Oobabooga API.
The function also saves the annotated responses as a CSV file in the "LLMoutput/columnname/model.csv" path.
}
\description{
This function sends text data to Oobabooga's API for annotation based on a provided system prompt.
The annotated responses are saved as a CSV file.
}
\examples{
\dontrun{
my_data <- c("Apple", "Tomato", "Broccoli")
annotated_data <- tag_oobabooga(my_data, sys_prompt = "Which one is a fruit?")
}
}
